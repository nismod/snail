{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and infrastructure exposure to flooding\n",
    "\n",
    "This notebook forms the basis of \"Hands-On 5\" in the CCG course.\n",
    "\n",
    "1. Extract infrastructure data from OpenStreetMap\n",
    "2. Extract flood hazard data from Aqueduct\n",
    "3. Intersect floods with roads to calculate exposure\n",
    "4. Open QGIS to look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Extract infrastructure data\n",
    "\n",
    "### Step 1) On your desktop, create a folder called `ghana_tutorial`\n",
    "\n",
    "### Step 2) Create a variable to store the folder location\n",
    "\n",
    "In the cell below, type in the path to your desktop, by changing NAME to match your username as shown on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit this if using a Mac (otherwise delete)\n",
    "data_folder = \"/Users/NAME/Desktop/ghana_tutorial\"\n",
    "\n",
    "# edit this if using Windows (otherwise delete)\n",
    "data_folder = \"C:\\\\Users\\\\NAME\\\\Desktop\\\\ghana_tutorial\"\n",
    "\n",
    "# delete this line\n",
    "data_folder = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Load Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The os and subprocess modules are built into Python\n",
    "# see https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# see https://docs.python.org/3/library/subprocess.html\n",
    "import subprocess\n",
    "\n",
    "# Pandas and GeoPandas are libraries for working with datasets\n",
    "# see https://geopandas.org/\n",
    "import geopandas as gpd\n",
    "\n",
    "gpd._compat.USE_PYGEOS = False\n",
    "# see https://pandas.pydata.org/\n",
    "import pandas as pd\n",
    "\n",
    "# snkit helps generate connected networks from lines and nodes\n",
    "# see https://snkit.readthedocs.io/\n",
    "import snkit\n",
    "import snkit.network\n",
    "\n",
    "# PyPROJ is a library for working with geographic projections\n",
    "# see https://pyproj4.github.io/\n",
    "from pyproj import Geod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4) and 5) Download and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `ghana-latest-free.shp.zip` dataset from http://download.geofabrik.de/africa/ghana.html, extract the zip folder and save the extracted folder within your new folder `ghana_tutorial`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6) Load the road dataset you've just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads = gpd.read_file(\n",
    "    os.path.join(\n",
    "        data_folder, \"ghana-latest-free.shp\", \"gis_osm_roads_free_1.shp\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7) To take a look at the data and the attribute table fill in and run the next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads.fclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8) Next we want to make a couple of changes to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out minor and residential roads, tracks and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the specified columns\n",
    "roads = roads[[\"osm_id\", \"fclass\", \"name\", \"geometry\"]]\n",
    "# Keep only the roads whose \"fclass\" is in the list\n",
    "roads = roads[\n",
    "    roads.fclass.isin(\n",
    "        [\n",
    "            \"motorway\",\n",
    "            \"motorway_link\",\n",
    "            \"trunk\",\n",
    "            \"trunk_link\",\n",
    "            \"primary\",\n",
    "            \"primary_link\",\n",
    "            \"secondary\",\n",
    "            \"secondary_link\",\n",
    "            \"tertiary\",\n",
    "            \"tertiary_link\",\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "# Rename some columns\n",
    "roads = roads.rename(\n",
    "    columns={\n",
    "        \"fclass\": \"road_type\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topological network information - this adds information that will let us find routes over the road network.\n",
    "- add nodes at the start and end of each road segment\n",
    "- split roads at junctions, so each segment goes from junction to junction\n",
    "- add ids to each node and edge, and add `from_id` and `to_id` to each edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_network = snkit.Network(edges=roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_endpoints = snkit.network.add_endpoints(road_network)\n",
    "split_edges = snkit.network.split_edges_at_nodes(with_endpoints)\n",
    "with_ids = snkit.network.add_ids(\n",
    "    split_edges, id_col=\"id\", edge_prefix=\"roade\", node_prefix=\"roadn\"\n",
    ")\n",
    "connected = snkit.network.add_topology(with_ids)\n",
    "roads = connected.edges\n",
    "road_nodes = connected.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the length of each road segment in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geod = Geod(ellps=\"WGS84\")\n",
    "roads[\"length_m\"] = roads.geometry.apply(geod.geometry_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads.crs = {\"init\": \"epsg:4326\"}\n",
    "road_nodes.crs = {\"init\": \"epsg:4326\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9) Save the pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads.to_file(\n",
    "    os.path.join(data_folder, \"GHA_OSM_roads.gpkg\"),\n",
    "    layer=\"edges\",\n",
    "    driver=\"GPKG\",\n",
    ")\n",
    "road_nodes.to_file(\n",
    "    os.path.join(data_folder, \"GHA_OSM_roads.gpkg\"),\n",
    "    layer=\"nodes\",\n",
    "    driver=\"GPKG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: Extract and polygonise hazard data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Download flood hazard data from Aqueduct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full [Aqueduct dataset](https://www.wri.org/resources/data-sets/aqueduct-floods-hazard-maps) is available to download openly. There are some scripts and summary of the data you may find useful at [nismod/aqueduct](https://github.com/nismod/aqueduct).\n",
    "\n",
    "There are almost 700 files in the full Aqueduct dataset, of up to around 100MB each, so we don't recommend downloading all of them unless you intend to do further analysis.\n",
    "\n",
    "For later tutorials, we provide a preprocessed set of hazard polygons for the Ghana example. \n",
    "\n",
    "The next steps show how to clip a region out of the global dataset and polygonise it, in case you wish to reproduce this analysis in another part of the world.\n",
    "\n",
    "For now, we suggest downloading [inunriver_historical_000000000WATCH_1980_rp00100.tif](http://wri-projects.s3.amazonaws.com/AqueductFloodTool/download/v2/inunriver_historical_000000000WATCH_1980_rp00100.tif) to work through the next steps. Save the downloaded file in a new folder titled `flood_layer` under your data_folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Run the code below to polygonise the tif files\n",
    "\n",
    "This converts the flood maps from *tiff files (raster data)* into *shape files (vector data)*. It will take a little time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = \"-3.262509\"\n",
    "ymin = \"4.737128\"\n",
    "xmax = \"1.187968\"\n",
    "ymax = \"11.162937\"\n",
    "\n",
    "for root, dirs, files in os.walk(data_folder, \"flood_layer\"):\n",
    "    print(\"Looking in\", root)\n",
    "    for file in sorted(files):\n",
    "        if file.endswith(\".tif\") and not file.endswith(\"p.tif\"):\n",
    "            print(\"Found tif file\", file)\n",
    "            stem = file[:-4]\n",
    "            input_file = os.path.join(root, file)\n",
    "\n",
    "            # Clip file to bounds\n",
    "            clip_file = os.path.join(root, f\"{stem}_clip.tif\")\n",
    "            try:\n",
    "                os.remove(clip_file)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            p = subprocess.run(\n",
    "                [\n",
    "                    \"gdalwarp\",\n",
    "                    \"-te\",\n",
    "                    xmin,\n",
    "                    ymin,\n",
    "                    xmax,\n",
    "                    ymax,\n",
    "                    input_file,\n",
    "                    clip_file,\n",
    "                ],\n",
    "                capture_output=True,\n",
    "            )\n",
    "            print(p.stdout.decode(\"utf8\"))\n",
    "            print(p.stderr.decode(\"utf8\"))\n",
    "            print(clip_file)\n",
    "\n",
    "            # Create vector outline of raster areas\n",
    "            # note that this rounds the floating-point values of flood depth from\n",
    "            # the raster to the nearest integer in the vector outlines\n",
    "            polygons_file = os.path.join(root, f\"{stem}.gpkg\")\n",
    "            try:\n",
    "                os.remove(polygons_file)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            p = subprocess.run(\n",
    "                [\n",
    "                    \"gdal_polygonize.py\",\n",
    "                    clip_file,\n",
    "                    \"-q\",\n",
    "                    \"-f\",\n",
    "                    \"GPKG\",\n",
    "                    polygons_file,\n",
    "                ],\n",
    "                capture_output=True,\n",
    "            )\n",
    "            print(p.stdout.decode(\"utf8\"))\n",
    "            print(p.stderr.decode(\"utf8\"))\n",
    "            print(polygons_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: Intersect hazard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now intersect the hazard and the roads, starting with one hazard initially so we save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Specify your input and output path as well as the name of the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_path = os.path.join(\n",
    "    data_folder,\n",
    "    \"flood_layer\",\n",
    "    \"inunriver_historical_000000000WATCH_1980_rp00100.gpkg\",\n",
    ")\n",
    "\n",
    "output_path = os.path.join(\n",
    "    data_folder,\n",
    "    \"results\",\n",
    "    \"inunriver_historical_000000000WATCH_1980_rp00100_exposure.gpkg\",\n",
    ")\n",
    "\n",
    "flood = gpd.read_file(flood_path).rename(columns={\"DN\": \"depth_m\"})\n",
    "flood = flood[flood.depth_m > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Run the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_intersections = gpd.overlay(GHA_OSM_roads, flood, how=\"intersection\")\n",
    "flood_intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the exposed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geod = Geod(ellps=\"WGS84\")\n",
    "flood_intersections[\"flood_length_m\"] = flood_intersections.geometry.apply(\n",
    "    geod.geometry_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_intersections.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the proportion of roads in our dataset which are exposed to >=1m flood depths in this scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposed_length = flood_intersections.flood_length_m.sum()\n",
    "exposed_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roads_in_dataset_length = GHA_OSM_roads.length_m.sum()\n",
    "all_roads_in_dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = exposed_length / all_roads_in_dataset_length\n",
    "proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{proportion:.0%} of roads in this dataset are exposed to flood depths of >= 1m in a historical 1-in-100 year flood\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to file (with spatial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_intersections.to_file(output_path, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to CSV (without spatial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_intersections.drop(columns=\"geometry\").to_csv(\n",
    "    output_path.replace(\".gpkg\", \".csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
